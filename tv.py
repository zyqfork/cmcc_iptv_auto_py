# -*- coding: utf-8 -*-
import json
import re
import os
import io
import requests
import sys
import gzip
import time 
import threading 
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom
from concurrent.futures import ThreadPoolExecutor, as_completed

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç 
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ===================== è‡ªå®šä¹‰é…ç½®åŒºåŸŸ =====================
#  EPG ä¸‹è½½é‡è¯•é…ç½®
EPG_DOWNLOAD_RETRY_COUNT = 3  # é‡è¯•æ¬¡æ•°
EPG_DOWNLOAD_RETRY_DELAY = 2  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
EPG_DOWNLOAD_TIMEOUT = 15     # å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# åœ¨è¿™é‡Œä¿®æ”¹è¾“å‡ºæ–‡ä»¶åï¼ˆä¿æŒé»˜è®¤å³å¯ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼‰
TV_M3U_FILENAME = "tv.m3u"        # ç»„æ’­åœ°å€åˆ—è¡¨æ–‡ä»¶
TV2_M3U_FILENAME = "tv2.m3u"      # è½¬å•æ’­åœ°å€åˆ—è¡¨æ–‡ä»¶
KU9_M3U_FILENAME = "ku9.m3u"      #  KU9å›çœ‹å‚æ•°æ ¼å¼æ–‡ä»¶
XML_FILENAME = "t.xml"            # XMLèŠ‚ç›®å•æ–‡ä»¶
REPLACEMENT_IP = "http://c.cc.top:7088/udp"  # UDPXYåœ°å€ï¼Œ
REPLACEMENT_IP_TV = ""  # tv.m3u ä¸“ç”¨çš„ UDPXY åœ°å€ï¼ˆé»˜è®¤ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹åœ°å€ï¼‰
CATCHUP_SOURCE_PREFIX = "http://183.235.162.80:6610/190000002005"  # å›çœ‹æºå‰ç¼€ï¼Œ
NGINX_PROXY_PREFIX = ""  # é’ˆå¯¹å¤–ç½‘æ’­æ”¾çš„nginxä»£ç†
ENABLE_NGINX_PROXY_FOR_TV = False  # tv.m3u æ˜¯å¦ä½¿ç”¨ NGINX_PROXY_PREFIX ä»£ç†ï¼ˆé»˜è®¤ Falseï¼‰
JSON_URL = "http://183.235.16.92:8082/epg/api/custom/getAllChannel.json" # JSON æ–‡ä»¶ä¸‹è½½ URL  è¿™ä¸ªåœ°å€æœ‰æ™´å½©

#  EPG åœ°å€é…ç½® - å¯è‡ªå®šä¹‰ä¿®æ”¹
M3U_EPG_URL = "https://epg.112114.xyz/pp.xml.gz"  # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™… EPG åœ°å€
# (æ–°å¢) EPG ä¸‹è½½æºåœ°å€ (å¯ä»¥é…ç½®å¤šä¸ª, ä»»åŠ¡ä¼šè‡ªåŠ¨åˆ†é…)
EPG_BASE_URLS = [
    "http://183.235.16.92:8082/epg/api/channel/",
    "http://183.235.11.39:8082/epg/api/channel/"
]
#  å›çœ‹å‚æ•°é…ç½® - å¯è‡ªå®šä¹‰ä¿®æ”¹
CATCHUP_URL_TEMPLATE = "{prefix}/{ztecode}/index.m3u8?starttime=${{utc:yyyyMMddHHmmss}}&endtime=${{utcend:yyyyMMddHHmmss}}"
#  æ·»åŠ KU9å›çœ‹æ¨¡æ¿
CATCHUP_URL_KU9 = "{prefix}/{ztecode}/index.m3u8?starttime=${{(b)yyyyMMddHHmmss|UTC}}&endtime=${{(e)yyyyMMddHHmmss|UTC}}"

# è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
CHANNEL_ORDER_FILE = "channel_order.json"        # é¢‘é“æ’åºæ–‡ä»¶
CUSTOM_CHANNELS_FILE = "custom_channels.json"    # è‡ªå®šä¹‰é¢‘é“æ–‡ä»¶

# å¤–éƒ¨ M3U åˆå¹¶é…ç½®
EXTERNAL_M3U_URL = "https://bc.188766.xyz/?ip=&mishitong=true&mima=mianfeibuhuaqian&json=true"  # å¤–éƒ¨ M3U ä¸‹è½½é“¾æ¥
EXTERNAL_GROUP_TITLES = ["ç²¤è¯­é¢‘é“"]  # è¦æå–çš„ group-title åˆ—è¡¨ï¼Œä¾‹å¦‚: ["å†°èŒ¶ä½“è‚²", "ç²¤è¯­é¢‘é“"]
ENABLE_EXTERNAL_M3U_MERGE = True  # æ˜¯å¦åˆå¹¶å¤–éƒ¨ M3U åˆ°æ‰€æœ‰ M3U æ–‡ä»¶ (True/False)

#  æ‰©å±•é»‘åå•é…ç½® - æ”¯æŒæŒ‰ titleã€code æˆ– zteurl è¿‡æ»¤
BLACKLIST_RULES = {
    "title": ["æµ‹è¯•é¢‘é“", "è´­ç‰©", "å¯¼è§†", "ç™¾è§†é€š", "æŒ‡å—", "ç²¾é€‰é¢‘é“"],
    "code": [
             # "02000006000000052022060699000003",
    ],
    "zteurl": [
              # "rtp://239.21.0.137:3892",
    ]
}

# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šè½¬æ¢ä¸ºé›†åˆ
BLACKLIST_TITLE_SET = set(BLACKLIST_RULES["title"])
BLACKLIST_CODE_SET = set(BLACKLIST_RULES["code"])
BLACKLIST_ZTEURL_SET = set(BLACKLIST_RULES["zteurl"])

# é¢‘é“åç§°æ˜ å°„ï¼ˆå°†é«˜æ¸…é¢‘é“æ˜ å°„åˆ°æ ‡å‡†åç§°ï¼‰
CHANNEL_NAME_MAP = {
    "CCTV-1é«˜æ¸…": "CCTV-1ç»¼åˆ",
    "CCTV-2é«˜æ¸…": "CCTV-2è´¢ç»",
    "CCTV-3é«˜æ¸…": "CCTV-3ç»¼è‰º",
    "CCTV-4é«˜æ¸…": "CCTV-4ä¸­æ–‡å›½é™…",
    "CCTV-5é«˜æ¸…": "CCTV-5ä½“è‚²",
    "CCTV-6é«˜æ¸…": "CCTV-6ç”µå½±",
    "CCTV-7é«˜æ¸…": "CCTV-7å›½é˜²å†›äº‹",
    "CCTV-8é«˜æ¸…": "CCTV-8ç”µè§†å‰§",
    "CCTV-9é«˜æ¸…": "CCTV-9çºªå½•",
    "CCTV-10é«˜æ¸…": "CCTV-10ç§‘æ•™",
    "CCTV-11é«˜æ¸…": "CCTV-11æˆæ›²",
    "CCTV-12é«˜æ¸…": "CCTV-12ç¤¾ä¼šä¸æ³•",
    "CCTV-13é«˜æ¸…": "CCTV-13æ–°é—»",
    "CCTV-14é«˜æ¸…": "CCTV-14å°‘å„¿é«˜æ¸…",
    "CCTV-15é«˜æ¸…": "CCTV-15éŸ³ä¹",
    "CCTV-16é«˜æ¸…": "CCTV-16å¥¥æ—åŒ¹å…‹",
    "CCTV-17é«˜æ¸…": "CCTV-17å†œä¸šé«˜æ¸…",
    "å¹¿å·æ–°é—»-æµ‹è¯•": "å¹¿å·æ–°é—»é«˜æ¸…",
    "å¹¿å·ç»¼åˆ-æµ‹è¯•": "å¹¿å·ç»¼åˆé«˜æ¸…"
}

# EPG ä¸‹è½½å¼€å…³:
ENABLE_EPG_DOWNLOAD = True  # True - å¯ç”¨EPGä¸‹è½½å’Œç”Ÿæˆ  False - è·³è¿‡EPGä¸‹è½½

# EPG ä¸‹è½½æ¨¡å¼:
EPG_DOWNLOAD_MODE = "M3U_ONLY"  # é»˜è®¤ä¿®æ”¹ä¸º "M3U_ONLY"  "M3U_ONLY"  - ä»…ä¸‹è½½å’Œåˆæˆ M3U æ–‡ä»¶ä¸­å®é™…åŒ…å«çš„é¢‘é“ )  "ALL" - ä¸‹è½½å’Œåˆæˆæ‰€æœ‰å¯ç”¨é¢‘é“ï¼ˆåŒ…æ‹¬è¢« M3U è¿‡æ»¤æ‰çš„)

# EPG åˆæˆæ¨¡å¼: 
# True  - (æ¨è) ä»…å½“é¢‘é“æœ‰èŠ‚ç›®æ•°æ®æ—¶æ‰å°†å…¶å†™å…¥ XMLã€‚è¿™å¯èƒ½å¯¼è‡´æ’­æ”¾å™¨æ— æ³•æ˜ å°„é¢‘é“åç§°ã€‚æ–¹ä¾¿iptoolæ•´åˆ  False - (ä¸æ¨è) å³ä½¿é¢‘é“æ²¡æœ‰èŠ‚ç›®æ•°æ®ä¹Ÿå†™å…¥ <channel> æ ‡ç­¾ (ç”¨äºé¢‘é“åç§°/å›¾æ ‡æ˜ å°„)ï¼Œåªæ˜¯ä¸åŒ…å« <programme> æ ‡ç­¾ã€‚
XML_SKIP_CHANNELS_WITHOUT_EPG = True # é»˜è®¤ä¸º True

# 1. å®šä¹‰æ‰€æœ‰åˆ†ç»„å’Œå®ƒä»¬çš„å…³é”®å­— (è¿™é‡Œçš„é¡ºåºä¸é‡è¦)
GROUP_DEFINITIONS = {
    "å¤®è§†": ["CCTV"],
    "å¤®è§†ç‰¹è‰²": ["å…µå™¨ç§‘æŠ€", "é£äº‘", "ç¬¬ä¸€å‰§åœº", "ä¸–ç•Œåœ°ç†", "å¤®è§†", "å«ç”Ÿå¥åº·", "æ€€æ—§", "å¥³æ€§", "é«˜å°”å¤«", "é‡‘é¹°çºªå®"],
    "å¹¿ä¸œ": ["å¹¿ä¸œ", "å¤§æ¹¾åŒº", "ç»æµç§‘æ•™", "å—æ–¹", "å²­å—", "ç°ä»£æ•™è‚²", "ç§»åŠ¨é¢‘é“"],
    "å«è§†": ["å«è§†"],
    "å°‘å„¿": ["å°‘å„¿", "å¡é€š", "åŠ¨ç”»", "æ•™è‚²"],
    "CGTN": ["CGTN"],
    "åæ•°å’ªå’•": ["çˆ±", "ç›å½©", "IPTV", "å’ªå’•", "çƒ­æ’­", "ç»å…¸", "é­…åŠ›"],
    "è¶…æ¸…4k": ["è¶…æ¸…", "4k", "4K"],
    "å¹¿ä¸œåœ°æ–¹å°": [],  # è‡ªå®šä¹‰é¢‘é“åˆ†ç»„ï¼Œæ²¡æœ‰å…³é”®å­—
    "å…¶ä»–": []          # ä¿åº•åˆ†ç»„ï¼Œæ²¡æœ‰å…³é”®å­—
}

# 2. å®šä¹‰åˆ†ç±»é€»è¾‘çš„ *ä¼˜å…ˆçº§* (e.g., "å°‘å„¿" å¿…é¡»åœ¨ "å¤®è§†" ä¹‹å‰) è¿™é‡Œçš„é¡ºåºå†³å®šä¸€ä¸ªé¢‘é“è¢«åˆ†åˆ°å“ªä¸ªç»„
GROUP_CLASSIFICATION_PRIORITY = [
    "å°‘å„¿",       # å¿…é¡»åœ¨ "å¤®è§†" å’Œ "å¹¿ä¸œ" ä¹‹å‰,ä¸ç„¶cctv14å°‘å„¿ ä¼šåˆ†åˆ°å¤®è§†,ç¡®ä¿æ¯ä¸ªåˆ†ç»„åªæœ‰ä¸€ä¸ªé¢‘é“,ä¸ä¼šæœ‰é‡å¤é¢‘é“
    "è¶…æ¸…4k",   # å¿…é¡»åœ¨ "å¤®è§†" ä¹‹å‰ï¼Œå¦åˆ™CCTV-4Kä¼šè¢«åˆ†åˆ°å¤®è§†
    "å¤®è§†",
    "å¤®è§†ç‰¹è‰²",
    "å¹¿ä¸œ",
    "CGTN",
    "å«è§†",
    "åæ•°å’ªå’•",
    # "å¹¿ä¸œåœ°æ–¹å°" å’Œ "å…¶ä»–" æ²¡æœ‰å…³é”®å­—ï¼Œä¸éœ€è¦åœ¨è¿™é‡Œ
]

# 3. å®šä¹‰ M3U å’Œ XML æ–‡ä»¶ä¸­çš„ *è¾“å‡ºé¡ºåº*  (ä½ å¯ä»¥éšæ„æ’åˆ—è¿™é‡Œçš„é¡ºåºï¼Œ"å°‘å„¿" é‡æ’åº)
GROUP_OUTPUT_ORDER = [
    "å¤®è§†",
    "ç²¤è¯­é¢‘é“",
    "å¹¿ä¸œ",
    "å¤®è§†ç‰¹è‰²",
    "å°‘å„¿",  # <--- "å°‘å„¿" é‡æ’åº
    "å«è§†",
    "åæ•°å’ªå’•",
    "CGTN",
    "è¶…æ¸…4k",
    "å…¶ä»–",
    "å¹¿ä¸œåœ°æ–¹å°"
]

# è‡ªåŠ¨ç”Ÿæˆçš„å‹ç¼©æ–‡ä»¶åï¼ˆåŸºäºXMLæ–‡ä»¶åï¼‰
XML_GZ_FILENAME = XML_FILENAME + ".gz"

# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
CCTV_PATTERN = re.compile(r'CCTV-(\d+)')  # åŒ¹é…CCTV-æ•°å­—æ¨¡å¼
NUMBER_PATTERN = re.compile(r'\d+')  # åŒ¹é…æ•°å­—
QUALITY_PATTERN = re.compile(r'(?:é«˜æ¸…|è¶…æ¸…|4K|\d+K)')  # åŒ¹é…æ¸…æ™°åº¦æ ‡è¯†
TVG_ID_CLEAN_PATTERN = re.compile(r'[_\s]*(é«˜æ¸…|è¶…æ¸…|4K)[_\s]*')  # æ¸…ç†tvg-idä¸­çš„æ¸…æ™°åº¦æ ‡è¯†
SPACE_DASH_PATTERN = re.compile(r'\s+-\s+')  # åŒ¹é…ç©ºæ ¼-ç©ºæ ¼æ¨¡å¼
MULTI_SPACE_PATTERN = re.compile(r'\s+')  # åŒ¹é…å¤šä¸ªç©ºæ ¼

# é­”æ³•å­—ç¬¦ä¸²å’Œæ•°å­—æå–ä¸ºå¸¸é‡
TIMEZONE_OFFSET = "+0800"  # æ—¶åŒºåç§»
DATE_FORMAT = "%Y%m%d"  # æ—¥æœŸæ ¼å¼
XML_GENERATOR_NAME = "Custom EPG Generator"  # XMLç”Ÿæˆå™¨åç§°
LOG_SEPARATOR = "=" * 50  # æ—¥å¿—åˆ†éš”ç¬¦
UNKNOWN_CHANNEL = "Unknown"  # æœªçŸ¥é¢‘é“åç§°
UNKNOWN_PROGRAMME = "Unknown Programme"  # æœªçŸ¥èŠ‚ç›®åç§°
CHANNEL_PROCESSING_LOG = "channel_processing.log"  # é¢‘é“å¤„ç†æ—¥å¿—æ–‡ä»¶å
EPG_STATISTICS_LOG = "epg_statistics.log"  # EPGç»Ÿè®¡æ—¥å¿—æ–‡ä»¶å

# è§„èŒƒåŒ–é…ç½®ï¼ˆç¨‹åºå†…éƒ¨ä½¿ç”¨ï¼‰
def normalize_url(url, trailing_slash='keep'):
    """
    è§„èŒƒåŒ–URLï¼Œç¡®ä¿æ–œæ å¤„ç†æ­£ç¡®
    :param url: è¦è§„èŒƒåŒ–çš„URL
    :param trailing_slash: 'keep' (é»˜è®¤), 'add' (æ·»åŠ æ–œæ ), or 'remove' (ç§»é™¤æ–œæ ).
    """
    if not url:
        return url
    
    if trailing_slash == 'add':
        if not url.endswith('/'):
            url += '/'
    elif trailing_slash == 'remove':
        if url.endswith('/'):
            url = url.rstrip('/')
            
    return url

def ensure_url_scheme(url, default_scheme='http'):
    """
    ç¡®ä¿URLåŒ…å«åè®®å‰ç¼€ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ é»˜è®¤åè®®
    å…¼å®¹ä¸åŒPythonç‰ˆæœ¬çš„urlparseè¡Œä¸º
    ä½¿ç”¨å­—ç¬¦ä¸²æ£€æŸ¥è€Œä¸æ˜¯urlparseï¼Œæ›´å¯é 
    :param url: è¦å¤„ç†çš„URL
    :param default_scheme: é»˜è®¤åè®®ï¼ˆé»˜è®¤ä¸º 'http'ï¼‰
    :return: åŒ…å«åè®®å‰ç¼€çš„URL
    """
    if not url:
        return url
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆé˜²æ­¢å…¶ä»–ç±»å‹ï¼‰
    url = str(url).strip()
    
    if not url:
        return url
    
    # å¦‚æœå·²ç»æœ‰åè®®å‰ç¼€ï¼ˆåŒ…å« ://ï¼‰ï¼Œç›´æ¥è¿”å›
    if '://' in url:
        return url
    
    # å¦‚æœæ²¡æœ‰åè®®å‰ç¼€ï¼Œæ·»åŠ é»˜è®¤åè®®
    # å…¼å®¹å¤„ç†ï¼šå»é™¤å¯èƒ½çš„å‰å¯¼æ–œæ 
    url = url.lstrip('/')
    if url:
        return f"{default_scheme}://{url}"
    else:
        return url
# åº”ç”¨è§„èŒƒåŒ–
REPLACEMENT_IP_NORM = normalize_url(REPLACEMENT_IP, trailing_slash='add')
REPLACEMENT_IP_TV_NORM = normalize_url(REPLACEMENT_IP_TV, trailing_slash='add') if REPLACEMENT_IP_TV else ""
CATCHUP_SOURCE_PREFIX_NORM = normalize_url(CATCHUP_SOURCE_PREFIX, trailing_slash='remove')
NGINX_PROXY_PREFIX_NORM = normalize_url(NGINX_PROXY_PREFIX, trailing_slash='add')

def clean_tvg_id(title):
    """æ¸…ç†é¢‘é“æ ‡é¢˜ï¼Œç”Ÿæˆæ ‡å‡†çš„ tvg-id"""
    cleaned = TVG_ID_CLEAN_PATTERN.sub('', title)
    if 'CCTV' in cleaned:
        cleaned = cleaned.replace('-', '')
    return cleaned.strip()

def apply_channel_name_mapping(channel, base_name):
    # å¦‚æœæ ‡é¢˜åœ¨æ˜ å°„è¡¨ä¸­ï¼Œç›´æ¥è¿”å›æ˜ å°„åçš„åç§°
    if channel["title"] in CHANNEL_NAME_MAP:
        return CHANNEL_NAME_MAP[channel["title"]]
    
    # å¯¹äºCCTVé¢‘é“ï¼Œä½¿ç”¨æ ‡å‡†åç§°
    cctv_match = CCTV_PATTERN.search(base_name)
    if cctv_match:
        cctv_num = cctv_match.group(1)
        # ä»åç§°æ˜ å°„ä¸­æŸ¥æ‰¾å¯¹åº”çš„æ ‡å‡†åç§°
        for key, value in CHANNEL_NAME_MAP.items():
            if f"CCTV-{cctv_num}" in key:
                return value
        return f"CCTV-{cctv_num}"
    
    return channel["title"]

def print_configuration():
    """æ‰“å°å½“å‰ä½¿ç”¨çš„é…ç½®"""
    print(f"ä½ çš„ç»„æ’­è½¬å•æ’­UDPXYåœ°å€æ˜¯ {REPLACEMENT_IP_NORM}")
    if REPLACEMENT_IP_TV_NORM:
        print(f"tv.m3u ä¸“ç”¨çš„UDPXYåœ°å€æ˜¯ {REPLACEMENT_IP_TV_NORM}")
    else:
        print(f"tv.m3u ä½¿ç”¨åŸå§‹åœ°å€ï¼ˆæœªé…ç½® REPLACEMENT_IP_TVï¼‰")
    print(f"ä½ çš„å›çœ‹æºå‰ç¼€æ˜¯ {CATCHUP_SOURCE_PREFIX_NORM}")
    print(f"ä½ çš„nginxä»£ç†å‰ç¼€æ˜¯ {NGINX_PROXY_PREFIX_NORM}")
    print(f"tv.m3u ä½¿ç”¨nginxä»£ç†: {'æ˜¯' if ENABLE_NGINX_PROXY_FOR_TV else 'å¦'}")
    print(f"ä½ çš„å›çœ‹URLæ¨¡æ¿æ˜¯ {CATCHUP_URL_TEMPLATE}")
    print(f"ä½ çš„KU9å›çœ‹URLæ¨¡æ¿æ˜¯ {CATCHUP_URL_KU9}")
    print(f"EPGä¸‹è½½å¼€å…³: {'å¯ç”¨' if ENABLE_EPG_DOWNLOAD else 'ç¦ç”¨'}")
    if ENABLE_EPG_DOWNLOAD:
        print(f"EPGä¸‹è½½é…ç½®: é‡è¯•{EPG_DOWNLOAD_RETRY_COUNT}æ¬¡, è¶…æ—¶{EPG_DOWNLOAD_TIMEOUT}ç§’, é—´éš”{EPG_DOWNLOAD_RETRY_DELAY}ç§’")
    print(f"å¤–éƒ¨M3Uåˆå¹¶å¼€å…³: {'å¯ç”¨' if ENABLE_EXTERNAL_M3U_MERGE else 'ç¦ç”¨'}")
    if ENABLE_EXTERNAL_M3U_MERGE:
        print(f"å¤–éƒ¨M3Uåœ°å€: {EXTERNAL_M3U_URL}")
        print(f"æå–çš„åˆ†ç»„: {', '.join(EXTERNAL_GROUP_TITLES) if EXTERNAL_GROUP_TITLES else '(æœªé…ç½®)'}")

def download_with_retry(url, max_retries=EPG_DOWNLOAD_RETRY_COUNT, timeout=EPG_DOWNLOAD_TIMEOUT, headers=None):
    """ å¸¦é‡è¯•æœºåˆ¶çš„ä¸‹è½½å‡½æ•°"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=timeout, headers=headers)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            error_type = type(e).__name__
            if attempt < max_retries - 1:
                print(f"  ä¸‹è½½æ—¶å‘ç”Ÿ '{error_type}' é”™è¯¯ï¼Œ{EPG_DOWNLOAD_RETRY_DELAY}ç§’åé‡è¯• ({attempt + 1}/{max_retries})...")
                time.sleep(EPG_DOWNLOAD_RETRY_DELAY)
            else:
                print(f"  ä¸‹è½½æ—¶å‘ç”Ÿ '{error_type}' é”™è¯¯ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                raise
    return None

def download_json_data(url):
    try:
        response = download_with_retry(url)
        data = response.json()
        print(f"æˆåŠŸè·å– JSON æ•°æ®ä» {url}")
        return data
    except requests.RequestException as e:
        print(f"ä¸‹è½½ JSON æ•°æ®å¤±è´¥: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"è§£æ JSON æ•°æ®å¤±è´¥: {e}")
        return None

def categorize_channel(title):
    """(é‡æ„) æ ¹æ® GROUP_CLASSIFICATION_PRIORITY åˆ—è¡¨çš„é¡ºåºä¸ºé¢‘é“åˆ†ç±»"""
    # æŒ‰ç…§ "åˆ†ç±»ä¼˜å…ˆçº§" åˆ—è¡¨çš„é¡ºåº
    for group_name in GROUP_CLASSIFICATION_PRIORITY:
        # ä» GROUP_DEFINITIONS è·å–è¯¥ç»„çš„å…³é”®å­—
        for keyword in GROUP_DEFINITIONS.get(group_name, []):
            if keyword in title:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„å°±è¿”å› è¿™ä¿è¯äº†ä¸€ä¸ªé¢‘é“åªåœ¨ä¸€ä¸ªåˆ†ç»„
                return group_name 
    
    # å¦‚æœæ‰€æœ‰å…³é”®å­—éƒ½æœªåŒ¹é…ï¼Œåˆ™å½’ç±»ä¸º"å…¶ä»–"
    return "å…¶ä»–"

def extract_number(title):
    match = NUMBER_PATTERN.search(title)
    return int(match.group()) if match else 0

def is_blacklisted(channel):
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆæ”¯æŒ titleã€codeã€zteurlï¼‰"""
    # æ£€æŸ¥æ ‡é¢˜é»‘åå•
    title = channel.get("title", "")
    if any(black_word in title for black_word in BLACKLIST_TITLE_SET):
        return True
    
    # æ£€æŸ¥ä»£ç é»‘åå•
    code = channel.get("code", "")
    if code in BLACKLIST_CODE_SET:
        return True
    
    # æ£€æŸ¥æ’­æ”¾é“¾æ¥é»‘åå•
    zteurl = channel.get("zteurl", "")
    if not zteurl:
        # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ zteurlï¼Œå°è¯•ä» params ä¸­è·å–
        params = channel.get("params", {})
        zteurl = params.get("zteurl", "") or params.get("hwurl", "")
    
    if zteurl in BLACKLIST_ZTEURL_SET:
        return True
    
    return False

def get_channel_base_name(title):
    """è·å–é¢‘é“çš„åŸºç¡€åç§°ï¼ˆæ”¹è¿›çš„CCTVé¢‘é“å¤„ç†ï¼‰"""
    # é¦–å…ˆå¤„ç†CCTVé¢‘é“çš„ç‰¹æ®Šæƒ…å†µ
    if "CCTV" in title:
        # åŒ¹é…CCTV-æ•°å­—çš„æ¨¡å¼
        cctv_match = CCTV_PATTERN.search(title)
        if cctv_match:
            cctv_num = cctv_match.group(1)
            # è¿”å›æ ‡å‡†åŒ–çš„CCTVåŸºç¡€åç§°
            return f"CCTV-{cctv_num}"
    
    # å¯¹äºéCCTVé¢‘é“ï¼Œå»é™¤å¸¸è§çš„é«˜æ¸…æ ‡è¯†
    base_name = QUALITY_PATTERN.sub('', title)
    # å»é™¤å¯èƒ½å¤šä½™çš„ç©ºæ ¼å’Œæ¨ªæ 
    base_name = SPACE_DASH_PATTERN.sub('', base_name)
    base_name = MULTI_SPACE_PATTERN.sub(' ', base_name)
    base_name = base_name.strip().strip('-').strip()
    return base_name

def get_channel_quality(title):
    """è·å–é¢‘é“çš„æ¸…æ™°åº¦"""
    if "è¶…æ¸…" in title or "4K" in title or "4k" in title:
        return "è¶…æ¸…"
    elif "é«˜æ¸…" in title:
        return "é«˜æ¸…"
    else:
        return "æ ‡æ¸…"

def is_cctv_channel(title):
    """æ£€æŸ¥æ˜¯å¦æ˜¯CCTVé¢‘é“"""
    return "CCTV" in title

def process_channels(channels):
    """å¤„ç†é¢‘é“åˆ—è¡¨ï¼Œè¿›è¡Œå»é‡å’Œåç§°æ˜ å°„"""
    # è¿‡æ»¤é»‘åå•é¢‘é“
    filtered_channels = []
    blacklisted_channels = []
    for channel in channels:
        if is_blacklisted(channel):
            blacklisted_channels.append({
                "title": channel["title"],
                "code": channel.get("code", ""),
                "reason": "é»‘åå•è§„åˆ™åŒ¹é…",
                "source": "ä¸»JSON"  #  æ·»åŠ æ¥æºæ ‡è¯†
            })
            continue
        filtered_channels.append(channel)
    
    print(f"å·²è¿‡æ»¤ {len(blacklisted_channels)} ä¸ªé»‘åå•é¢‘é“ï¼ˆä¸»JSONï¼‰")
    
    # æŒ‰åŸºç¡€åç§°åˆ†ç»„
    channel_groups = {}
    for channel in filtered_channels:
        base_name = get_channel_base_name(channel["title"])
        if base_name not in channel_groups:
            channel_groups[base_name] = []
        channel_groups[base_name].append(channel)
    
    # å¤„ç†æ¯ä¸ªé¢‘é“ç»„
    kept_channels = []
    removed_channels = []
    
    for base_name, group in channel_groups.items():
        # å¦‚æœåªæœ‰ä¸€ä¸ªé¢‘é“ï¼Œä¿ç•™å®ƒ
        if len(group) == 1:
            channel = group[0]
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åº”ç”¨åç§°æ˜ å°„
            if channel["title"] in CHANNEL_NAME_MAP:
                channel["final_name"] = CHANNEL_NAME_MAP[channel["title"]]
            else:
                channel["final_name"] = channel["title"]
            kept_channels.append(channel)
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯CCTVé¢‘é“ç»„
        is_cctv_group = any(is_cctv_channel(ch["title"]) for ch in group)
        
        if is_cctv_group:
            # å¯¹äºCCTVé¢‘é“ï¼Œä¼˜å…ˆä¿ç•™é«˜æ¸…ç‰ˆæœ¬
            hd_channels = [ch for ch in group if get_channel_quality(ch["title"]) == "é«˜æ¸…"]
            ultra_hd_channels = [ch for ch in group if get_channel_quality(ch["title"]) == "è¶…æ¸…"]
            
            # å¦‚æœæœ‰è¶…æ¸…ç‰ˆæœ¬ï¼Œä¼˜å…ˆä¿ç•™è¶…æ¸…
            if ultra_hd_channels:
                for channel in ultra_hd_channels:
                    # åº”ç”¨åç§°æ˜ å°„
                    channel["final_name"] = apply_channel_name_mapping(channel, base_name)
                    kept_channels.append(channel)
                
                # è®°å½•è¢«å‰”é™¤çš„å…¶ä»–ç‰ˆæœ¬
                for channel in group:
                    if channel not in ultra_hd_channels:
                        removed_channels.append({
                            "name": channel["title"],
                            "reason": f"CCTVé¢‘é“æœ‰è¶…æ¸…ç‰ˆæœ¬: {[ch['title'] for ch in ultra_hd_channels]}"
                        })
            
            # å¦‚æœæ²¡æœ‰è¶…æ¸…ä½†æœ‰é«˜æ¸…ç‰ˆæœ¬ï¼Œä¿ç•™é«˜æ¸…ç‰ˆæœ¬
            elif hd_channels:
                for channel in hd_channels:
                    # åº”ç”¨åç§°æ˜ å°„
                    channel["final_name"] = apply_channel_name_mapping(channel, base_name)
                    kept_channels.append(channel)
                
                # è®°å½•è¢«å‰”é™¤çš„æ ‡æ¸…CCTVé¢‘é“
                for channel in group:
                    if get_channel_quality(channel["title"]) == "æ ‡æ¸…":
                        removed_channels.append({
                            "name": channel["title"],
                            "reason": f"CCTVé¢‘é“æœ‰é«˜æ¸…ç‰ˆæœ¬: {[ch['title'] for ch in hd_channels]}"
                        })
            else:
                # æ²¡æœ‰é«˜æ¸…/è¶…æ¸…ç‰ˆæœ¬ï¼Œä¿ç•™æ‰€æœ‰æ ‡æ¸…ç‰ˆæœ¬
                for channel in group:
                    channel["final_name"] = channel["title"]
                    kept_channels.append(channel)
        else:
            # éCCTVé¢‘é“ç»„ï¼ŒæŒ‰åŸæ¥çš„é€»è¾‘å¤„ç†
            # æ‰¾å‡ºæ‰€æœ‰é«˜æ¸…/è¶…æ¸…ç‰ˆæœ¬
            hd_channels = [ch for ch in group if get_channel_quality(ch["title"]) in ["é«˜æ¸…", "è¶…æ¸…"]]
            
            # å¦‚æœæ²¡æœ‰é«˜æ¸…/è¶…æ¸…ç‰ˆæœ¬ï¼Œä¿ç•™æ‰€æœ‰æ ‡æ¸…ç‰ˆæœ¬
            if not hd_channels:
                for channel in group:
                    channel["final_name"] = channel["title"]
                    kept_channels.append(channel)
                continue
            
            # æœ‰é«˜æ¸…/è¶…æ¸…ç‰ˆæœ¬ï¼Œåªä¿ç•™è¿™äº›ç‰ˆæœ¬
            for channel in hd_channels:
                channel["final_name"] = channel["title"]
                kept_channels.append(channel)
            
            # è®°å½•è¢«å‰”é™¤çš„æ ‡æ¸…é¢‘é“
            for channel in group:
                if get_channel_quality(channel["title"]) == "æ ‡æ¸…":
                    removed_channels.append({
                        "name": channel["title"],
                        "reason": f"æœ‰é«˜æ¸…/è¶…æ¸…ç‰ˆæœ¬: {[ch['title'] for ch in hd_channels]}"
                    })
    
    # ä¸å†åœ¨è¿™é‡Œç”Ÿæˆæ—¥å¿—æ–‡ä»¶ï¼Œæ”¹ä¸ºåœ¨ main å‡½æ•°ä¸­ç»Ÿä¸€ç”Ÿæˆ
    return kept_channels, blacklisted_channels, removed_channels

def convert_time_to_xmltv_format(time_str):
    try:
        return f"{time_str} {TIMEZONE_OFFSET}"
    except ValueError as e:
        print(f"æ—¶é—´æ ¼å¼è½¬æ¢å¤±è´¥: {time_str}, é”™è¯¯: {e}")
        return None

def load_custom_channels(file_path):
    """åŠ è½½è‡ªå®šä¹‰é¢‘é“"""
    if not os.path.exists(file_path):
        print(f"è‡ªå®šä¹‰é¢‘é“æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            custom_channels = json.load(f)
        print(f"æˆåŠŸåŠ è½½è‡ªå®šä¹‰é¢‘é“æ–‡ä»¶: {file_path}")
        return custom_channels
    except Exception as e:
        print(f"åŠ è½½è‡ªå®šä¹‰é¢‘é“æ–‡ä»¶å¤±è´¥: {e}")
        return {}

def load_channel_order(file_path):
    """åŠ è½½é¢‘é“æ’åºé…ç½®"""
    if not os.path.exists(file_path):
        print(f"é¢‘é“æ’åºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            channel_order = json.load(f)
        print(f"æˆåŠŸåŠ è½½é¢‘é“æ’åºæ–‡ä»¶: {file_path}")
        return channel_order
    except Exception as e:
        print(f"åŠ è½½é¢‘é“æ’åºæ–‡ä»¶å¤±è´¥: {e}")
        return {}

def apply_custom_sorting(grouped_channels, channel_order):
    """åº”ç”¨è‡ªå®šä¹‰æ’åº"""
    for group_name, channels in grouped_channels.items():
        if group_name in channel_order:
            # è·å–è¯¥ç»„çš„æ’åºé…ç½®
            order_list = channel_order[group_name]
            
            # åˆ›å»ºé¢‘é“åç§°åˆ°é¢‘é“å¯¹è±¡çš„æ˜ å°„
            channel_map = {ch["title"]: ch for ch in channels}
            processed = set()  # ä½¿ç”¨é›†åˆè·Ÿè¸ªå·²å¤„ç†çš„é¢‘é“
            
            # æŒ‰ç…§é…ç½®çš„é¡ºåºé‡æ–°æ’åˆ—
            sorted_channels = []
            for channel_name in order_list:
                if channel_name in channel_map:
                    sorted_channels.append(channel_map[channel_name])
                    processed.add(channel_name)
            
            # æ·»åŠ æœªåœ¨æ’åºé…ç½®ä¸­æŒ‡å®šçš„é¢‘é“ï¼ˆæŒ‰åŸé¡ºåºï¼‰
            for remaining_channel in channels:
                if remaining_channel["title"] not in processed:
                    sorted_channels.append(remaining_channel)
            
            grouped_channels[group_name] = sorted_channels
    
    return grouped_channels

def add_custom_channels(grouped_channels, custom_channels):
    """æ·»åŠ è‡ªå®šä¹‰é¢‘é“åˆ°åˆ†ç»„ï¼Œè¿”å›ï¼ˆæ›´æ–°åçš„åˆ†ç»„ï¼Œé»‘åå•é¢‘é“åˆ—è¡¨ï¼Œå·²æ·»åŠ çš„è‡ªå®šä¹‰é¢‘é“åˆ—è¡¨ï¼‰"""
    blacklisted_custom_channels = []  #  è®°å½•è¢«è¿‡æ»¤çš„è‡ªå®šä¹‰é¢‘é“
    added_custom_channels = []  # è®°å½•æˆåŠŸæ·»åŠ çš„è‡ªå®šä¹‰é¢‘é“
    
    for group_name, channels in custom_channels.items():
        if group_name not in grouped_channels:
            print(f"è­¦å‘Š: è‡ªå®šä¹‰åˆ†ç»„ '{group_name}' æœªåœ¨ GROUP_DEFINITIONS ä¸­å®šä¹‰ï¼Œå°†è‡ªåŠ¨åˆ›å»ºã€‚")
            grouped_channels[group_name] = []
        
        for custom_channel in channels:
            # æ£€æŸ¥è‡ªå®šä¹‰é¢‘é“æ˜¯å¦åœ¨é»‘åå•ä¸­
            if is_blacklisted(custom_channel):
                blacklisted_info = {
                    "title": custom_channel.get('title', 'æœªçŸ¥'),
                    "code": custom_channel.get('code', ''),
                    "reason": "é»‘åå•è§„åˆ™åŒ¹é…", 
                    "source": "è‡ªå®šä¹‰é¢‘é“"  #  æ·»åŠ æ¥æºæ ‡è¯†
                }
                blacklisted_custom_channels.append(blacklisted_info)
                print(f"è·³è¿‡é»‘åå•ä¸­çš„è‡ªå®šä¹‰é¢‘é“: {custom_channel.get('title', 'æœªçŸ¥')}")
                continue
            
            # --- ä¿®æ”¹å¼€å§‹ ---
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åº”ç”¨åç§°æ˜ å°„
            original_title = custom_channel["title"]
            if original_title in CHANNEL_NAME_MAP:
                final_name = CHANNEL_NAME_MAP[original_title]
                print(f"è‡ªå®šä¹‰é¢‘é“åç§°æ˜ å°„: '{original_title}' -> '{final_name}'")
            else:
                final_name = original_title

            # ä¸ºè‡ªå®šä¹‰é¢‘é“æ·»åŠ å¿…è¦çš„å­—æ®µ
            custom_channel["title"] = final_name           # ä½¿ç”¨æœ€ç»ˆåç§°
            custom_channel["original_title"] = original_title  # ä¿ç•™åŸå§‹åç§°
            custom_channel["number"] = extract_number(final_name) # ä½¿ç”¨æœ€ç»ˆåç§°æå–ç¼–å·
            # --- ä¿®æ”¹ç»“æŸ ---
            
            custom_channel["is_custom"] = True  # æ ‡è®°ä¸ºè‡ªå®šä¹‰é¢‘é“
            
            # æ·»åŠ åˆ°åˆ†ç»„
            grouped_channels[group_name].append(custom_channel)
            # è®°å½•æˆåŠŸæ·»åŠ çš„è‡ªå®šä¹‰é¢‘é“
            added_custom_channels.append({
                "title": final_name,
                "original_title": original_title,
                "group": group_name
            })
    
    #  è¿”å›é»‘åå•ä¿¡æ¯å’Œå·²æ·»åŠ çš„é¢‘é“åˆ—è¡¨
    return grouped_channels, blacklisted_custom_channels, added_custom_channels

def download_epg_for_source(channels, base_url, total_channels, progress_counter, progress_lock):
    """
    (æ–°å¢) ä¸‹è½½å·¥ä½œå‡½æ•°ï¼šä»æŒ‡å®šçš„ base_url ä¸‹è½½ä¸€ç»„é¢‘é“çš„ EPG æ•°æ®ã€‚
    åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œã€‚
    """
    schedules_for_source = {}
    # ä¼˜åŒ–ï¼šåœ¨å‡½æ•°å¼€å§‹æ—¶è®¡ç®—ä¸€æ¬¡æ—¥æœŸï¼Œé¿å…é‡å¤è®¡ç®—
    now = datetime.now()
    current_date = now.strftime(DATE_FORMAT)
    next_date = (now + timedelta(days=1)).strftime(DATE_FORMAT)

    for channel in channels:
        code = channel["code"]
        
        # ä¸ºå½“å¤©å’Œç¬¬äºŒå¤©ç”Ÿæˆä¸‹è½½URL
        urls_for_channel = [
            f"{base_url}{code}.json?begintime={current_date}",
            f"{base_url}{code}.json?begintime={next_date}"
        ]
        
        for url in urls_for_channel:
            try:
                response = download_with_retry(url)
                data = response.json()
                
                if code not in schedules_for_source:
                    schedules_for_source[code] = {
                        "channel": data.get("channel", {}),
                        "schedules": []
                    }
                schedules_for_source[code]["schedules"].extend(data.get("schedules", []))
            except Exception as e:
                # åœ¨çº¿ç¨‹ä¸­æ‰“å°é”™è¯¯ï¼Œé¿å…ä¸­æ–­å…¶ä»–çº¿ç¨‹
                # (è¿™ä¸ª \n ä¼šå¦èµ·ä¸€è¡Œï¼Œä¿ç•™é”™è¯¯ï¼Œè¿›åº¦æ¡ä¼šåœ¨ä¸‹ä¸€è¡Œç»§ç»­)
                print(f"\nå¤„ç† {url} å¤±è´¥ (çº¿ç¨‹å†…): {e}")
        
        # --- å…³é”®ä¿®æ”¹ï¼šå¤„ç†å®Œä¸€ä¸ªé¢‘é“åï¼Œæ›´æ–°è¿›åº¦æ¡ ---
        with progress_lock:
            progress_counter[0] += 1  # å¢åŠ å…±äº«è®¡æ•°å™¨
            count = progress_counter[0]
            percent = (count / total_channels) * 100
            print(f"  ä¸‹è½½è¿›åº¦: {count}/{total_channels} ä¸ªé¢‘é“ ({percent:.1f}%)", end="\r", flush=True)
            
    return schedules_for_source

def _download_epg_data_parallel(channels_for_xml):
    """(Helper) å¹¶è¡Œä¸‹è½½æ‰€æœ‰é¢‘é“çš„EPGæ•°æ®ã€‚"""
    all_channels_to_download = [channel for group in channels_for_xml.values() for channel in group]
    num_channels = len(all_channels_to_download)
    num_sources = len(EPG_BASE_URLS)

    if num_sources == 0:
        print("é”™è¯¯: EPG_BASE_URLS é…ç½®ä¸ºç©ºï¼Œæ— æ³•ä¸‹è½½èŠ‚ç›®å•ã€‚")
        return {}

    chunk_size = (num_channels + num_sources - 1) // num_sources
    tasks = []
    for i in range(num_sources):
        start_index = i * chunk_size
        end_index = start_index + chunk_size
        channel_chunk = all_channels_to_download[start_index:end_index]
        if channel_chunk:
            tasks.append({"channels": channel_chunk, "base_url": EPG_BASE_URLS[i]})

    print(f"å‡†å¤‡å¹¶è¡Œä¸‹è½½ {num_channels} ä¸ªé¢‘é“çš„EPGï¼Œä½¿ç”¨ {len(tasks)} ä¸ªepgåœ°å€ä¸‹è½½...")

    progress_lock = threading.Lock()
    progress_counter = [0]
    all_schedules = {}

    with ThreadPoolExecutor(max_workers=len(tasks)) as executor:
        future_to_task = {
            executor.submit(download_epg_for_source, task["channels"], task["base_url"], num_channels, progress_counter, progress_lock): task
            for task in tasks
        }
        for future in as_completed(future_to_task):
            try:
                result = future.result()
                all_schedules.update(result)
            except Exception as exc:
                print(f'\nä¸€ä¸ªä¸‹è½½ä»»åŠ¡ç”Ÿæˆäº†å¼‚å¸¸: {exc}')
    
    print("\næ‰€æœ‰ä¸‹è½½ä»»åŠ¡å·²å®Œæˆã€‚")
    return all_schedules

def _build_xmltv_tree(channels_for_xml, all_schedules):
    """(Helper) æ ¹æ®EPGæ•°æ®æ„å»ºXMLTV ElementTreeã€‚"""
    root = ET.Element("tv")
    root.set("generator-info-name", XML_GENERATOR_NAME)
    
    stats = {
        "channels_in_xml": 0, "channels_with_epg": 0, "total_programmes": 0,
        "skipped_no_epg": 0, "with_epg_list": [], "without_epg_in_xml_list": [],
        "without_epg_skipped_list": []
    }

    for group in GROUP_OUTPUT_ORDER:
        if group in channels_for_xml:
            for channel_entry in channels_for_xml[group]:
                code = channel_entry["code"]
                channel_name = channel_entry["title"]
                
                schedules = all_schedules.get(code, {}).get("schedules", [])
                has_schedules = bool(schedules)

                if XML_SKIP_CHANNELS_WITHOUT_EPG and not has_schedules:
                    stats["skipped_no_epg"] += 1
                    stats["without_epg_skipped_list"].append(f"{channel_name} ({code})")
                    continue

                stats["channels_in_xml"] += 1
                if has_schedules:
                    stats["channels_with_epg"] += 1
                    stats["with_epg_list"].append(f"{channel_name} ({code})")
                else:
                    stats["without_epg_in_xml_list"].append(f"{channel_name} ({code})")

                channel_info = all_schedules.get(code, {}).get("channel", {})
                channel = ET.SubElement(root, "channel")
                channel_id = clean_tvg_id(channel_entry.get("original_title", channel_name))
                channel.set("id", channel_id)
                
                display_name = ET.SubElement(channel, "display-name")
                # ä½¿ç”¨ä¸M3U tvg-nameç›¸åŒçš„é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨original_title
                display_name.text = channel_entry.get("original_title", channel_entry.get("title", channel_info.get("title", UNKNOWN_CHANNEL)))

                if has_schedules:
                    for schedule in schedules:
                        stats["total_programmes"] += 1
                        programme = ET.SubElement(root, "programme")
                        programme.set("channel", channel_id)
                        
                        start = convert_time_to_xmltv_format(schedule.get("starttime", ""))
                        end = convert_time_to_xmltv_format(schedule.get("endtime", ""))
                        if start and end:
                            programme.set("start", start)
                            programme.set("stop", end)

                        title = ET.SubElement(programme, "title")
                        title.set("lang", "zh")
                        title.text = schedule.get("title", UNKNOWN_PROGRAMME)
    return root, stats

def _write_epg_files_and_stats(root, stats, output_file=XML_FILENAME):
    """(Helper) å°†XMLæ ‘å†™å…¥æ–‡ä»¶ï¼Œå‹ç¼©å¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    xml_str = minidom.parseString(ET.tostring(root, encoding='utf-8')).toprettyxml(indent="  ")
    
    # å†™å…¥XMLæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    print(f"å·²ä¿å­˜èŠ‚ç›®å•XMLæ–‡ä»¶åˆ°: {os.path.abspath(output_file)}")

    # ä¼˜åŒ–ï¼šç›´æ¥å‹ç¼©å†…å­˜ä¸­çš„å­—ç¬¦ä¸²ï¼Œé¿å…é‡å¤è¯»å–æ–‡ä»¶
    xml_bytes = xml_str.encode('utf-8')
    with gzip.open(XML_GZ_FILENAME, 'wb') as f_out:
        f_out.write(xml_bytes)
    print(f"å·²ç”Ÿæˆå‹ç¼©æ–‡ä»¶: {os.path.abspath(XML_GZ_FILENAME)}")

    # æ‰“å°å’Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
    print("\n" + LOG_SEPARATOR)
    print("EPG åˆæˆç»Ÿè®¡")
    print(LOG_SEPARATOR)
    print(f"\nåŸºæœ¬ç»Ÿè®¡:")
    print(f"   - XML ä¸­æ€»å…±å†™å…¥ {stats['channels_in_xml']} ä¸ªé¢‘é“")
    print(f"   - å…¶ä¸­ {stats['channels_with_epg']} ä¸ªé¢‘é“æˆåŠŸåˆæˆäº†èŠ‚ç›®æ•°æ®")
    print(f"   - æ€»å…±åˆæˆäº† {stats['total_programmes']} ä¸ªèŠ‚ç›®æ¡ç›®")
    if XML_SKIP_CHANNELS_WITHOUT_EPG:
        print(f"   - å·²è·³è¿‡ {stats['skipped_no_epg']} ä¸ªæ²¡æœ‰èŠ‚ç›®æ•°æ®çš„é¢‘é“")

    with open(EPG_STATISTICS_LOG, "w", encoding="utf-8") as f:
        f.write(f"EPG åˆæˆè¯¦ç»†ç»Ÿè®¡\n{LOG_SEPARATOR}\n\n")
        f.write(f"åŸºæœ¬ç»Ÿè®¡:\n")
        f.write(f"- XML ä¸­æ€»å…±å†™å…¥ {stats['channels_in_xml']} ä¸ªé¢‘é“\n")
        f.write(f"- å…¶ä¸­ {stats['channels_with_epg']} ä¸ªé¢‘é“æˆåŠŸåˆæˆäº†èŠ‚ç›®æ•°æ®\n")
        f.write(f"- æ€»å…±åˆæˆäº† {stats['total_programmes']} ä¸ªèŠ‚ç›®æ¡ç›®\n")
        if XML_SKIP_CHANNELS_WITHOUT_EPG:
            f.write(f"- å·²è·³è¿‡ {stats['skipped_no_epg']} ä¸ªæ²¡æœ‰èŠ‚ç›®æ•°æ®çš„é¢‘é“\n")
        
        f.write(f"\næœ‰ EPG æ•°æ®çš„é¢‘é“ ({len(stats['with_epg_list'])} ä¸ª):\n")
        for channel in sorted(stats['with_epg_list']):
            f.write(f"âœ“ {channel}\n")
        
        f.write(f"\næ²¡æœ‰ EPG æ•°æ®ä½†å·²åˆæˆåˆ° XML çš„é¢‘é“ ({len(stats['without_epg_in_xml_list'])} ä¸ª):\n")
        for channel in sorted(stats['without_epg_in_xml_list']):
            f.write(f"â—‹ {channel}\n")
        
        if XML_SKIP_CHANNELS_WITHOUT_EPG:
            f.write(f"\næ²¡æœ‰ EPG æ•°æ®ä¸”è¢«è·³è¿‡çš„é¢‘é“ ({len(stats['without_epg_skipped_list'])} ä¸ª):\n")
            for channel in sorted(stats['without_epg_skipped_list']):
                f.write(f"âœ— {channel}\n")
    
    print(f"\nè¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜åˆ°: {os.path.abspath(EPG_STATISTICS_LOG)}")
    print(LOG_SEPARATOR)

def download_and_save_all_schedules(channels_for_xml, output_file=XML_FILENAME):
    # 1. å¹¶è¡Œä¸‹è½½EPGæ•°æ®
    all_schedules = _download_epg_data_parallel(channels_for_xml)
    
    # 2. æ„å»ºXMLæ ‘å’Œç»Ÿè®¡æ•°æ®
    xml_tree, stats = _build_xmltv_tree(channels_for_xml, all_schedules)
    
    # 3. å†™å…¥æ–‡ä»¶å¹¶è®°å½•æ—¥å¿—
    _write_epg_files_and_stats(xml_tree, stats, output_file)

def run_epg_download(channels, custom_channels_config, grouped_channels):
    print("\nå¼€å§‹ä¸‹è½½èŠ‚ç›®å•...")
    
    all_channels_for_epg_download = [] # ç”¨äºç”Ÿæˆä¸‹è½½URLçš„åˆ—è¡¨
    channels_to_write_to_xml = {}      # ç”¨äºå†™å…¥XMLçš„é¢‘é“å­—å…¸ (å¸¦åˆ†ç»„)
      
    if EPG_DOWNLOAD_MODE == "M3U_ONLY":
        print("EPG æ¨¡å¼: M3U_ONLY (ä»…ä¸‹è½½å’Œåˆæˆ M3U ä¸­çš„é¢‘é“)")
        
        # 1. å†³å®šä¸‹è½½åˆ—è¡¨ï¼šéå† M3U é¢‘é“ (grouped_channels)
        for group_name, channels_in_group in grouped_channels.items():
            for channel in channels_in_group:
                # åªéœ€è¦ 'code' å³å¯
                if 'code' in channel:
                    all_channels_for_epg_download.append(channel)
        
        # 2. å†³å®šå†™å…¥XMLçš„åˆ—è¡¨ï¼šå°±æ˜¯ M3U åˆ—è¡¨
        channels_to_write_to_xml = grouped_channels
        
        m3u_channel_count = len(all_channels_for_epg_download)
        print(f"æ€»å…±å°†ä¸º {m3u_channel_count} ä¸ª M3U é¢‘é“æ¡ç›®å°è¯•ä¸‹è½½EPGã€‚")
        print(f"XML æ–‡ä»¶å°†åŸºäºè¿™ {m3u_channel_count} ä¸ªé¢‘é“ç”Ÿæˆã€‚")

    else: # é»˜è®¤ä¸º "ALL" æ¨¡å¼ (åŸè„šæœ¬çš„è¡Œä¸º)
        print("EPG æ¨¡å¼: ALL (ä¸‹è½½æ‰€æœ‰å¯ç”¨çš„é¢‘é“ï¼Œå¹¶å…¨éƒ¨å†™å…¥ XML)")
        
        # 1. å†³å®šä¸‹è½½åˆ—è¡¨ï¼š(åŸå§‹åˆ—è¡¨ + è‡ªå®šä¹‰åˆ—è¡¨)
        all_channels_for_epg_download = list(channels) # ä»ä¸»åˆ—è¡¨å¼€å§‹ (222ä¸ª)
        
        custom_channels_for_epg = []
        for group_name, custom_list in custom_channels_config.items():
            for custom_channel in custom_list:
                if 'code' in custom_channel:
                    custom_channels_for_epg.append(custom_channel)
                else:
                    print(f"è­¦å‘Š: è‡ªå®šä¹‰é¢‘é“ {custom_channel.get('title', 'N/A')} ç¼ºå°‘ 'code'ï¼Œæ— æ³•è·å–EPGã€‚")

        all_channels_for_epg_download.extend(custom_channels_for_epg) # (32ä¸ª)
        print(f"æ€»å…±å°†ä¸º {len(all_channels_for_epg_download)} ä¸ªé¢‘é“æ¡ç›®å°è¯•ä¸‹è½½EPGã€‚ (åŸå§‹+è‡ªå®šä¹‰)")

        # 2. å†³å®šå†™å…¥XMLçš„åˆ—è¡¨ï¼š(éœ€è¦é‡æ–°å¤„ç†æ‰€æœ‰é¢‘é“)
        print(f"æ­£åœ¨ä¸º XML (ALL æ¨¡å¼) é‡æ–°å¤„ç† {len(all_channels_for_epg_download)} ä¸ªé¢‘é“...")
        
        for channel in all_channels_for_epg_download:
            if "title" in channel and "code" in channel:
                original_title = channel["title"]
                # åº”ç”¨åç§°æ˜ å°„
                final_name = CHANNEL_NAME_MAP.get(original_title, original_title)
                
                # ä½¿ç”¨ original_title è¿›è¡Œåˆ†ç±»
                category = categorize_channel(original_title)
                
                # æ„å»ºç”¨äºXMLçš„ç²¾ç®€å¯¹è±¡
                channel_obj = {
                    "title": final_name,
                    "original_title": original_title,
                    "code": channel["code"],
                    "icon": channel.get("icon", ""),
                }
                
                if category not in channels_to_write_to_xml:
                    channels_to_write_to_xml[category] = []
                channels_to_write_to_xml[category].append(channel_obj)
            
        total_xml_channels = sum(len(v) for v in channels_to_write_to_xml.values())
        print(f"XML æ–‡ä»¶å°†åŒ…å« {total_xml_channels} ä¸ªé¢‘é“ (åŒ…æ‹¬è¢« M3U è¿‡æ»¤çš„)ã€‚")
    
    # ä½¿ç”¨ 'channels_to_write_to_xml' åˆ—è¡¨æ¥ç”Ÿæˆ XML
    download_and_save_all_schedules(channels_to_write_to_xml)
    # --- EPG å‡½æ•°å†…å®¹ç»“æŸ ---

def download_external_m3u(url):
    try:
        print(f"æ­£åœ¨ä¸‹è½½å¤–éƒ¨ M3U æ–‡ä»¶: {url}")
        # æ¨¡æ‹Ÿæµè§ˆå™¨çš„ HTTP å¤´éƒ¨ä¿¡æ¯ï¼Œé¿å… 403 Forbidden é”™è¯¯
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        response = download_with_retry(url, max_retries=3, timeout=30, headers=headers)
        if response:
            content = response.text
            print(f"æˆåŠŸä¸‹è½½å¤–éƒ¨ M3U æ–‡ä»¶ï¼Œå¤§å°: {len(content)} å­—èŠ‚")
            return content
        return None
    except Exception as e:
        print(f"ä¸‹è½½å¤–éƒ¨ M3U æ–‡ä»¶å¤±è´¥: {e}")
        return None

def parse_m3u_content(m3u_content, target_groups):
    """
    è§£æ M3U å†…å®¹ï¼Œæå–æŒ‡å®š group-title çš„é¢‘é“ï¼Œå¹¶åº”ç”¨é»‘åå•è¿‡æ»¤
    """
    if not m3u_content or not target_groups:
        return [], []
    
    # å°†ç›®æ ‡åˆ†ç»„è½¬æ¢ä¸ºé›†åˆï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡
    target_groups_set = set(target_groups)
    
    channels = []
    blacklisted_channels = []
    lines = m3u_content.strip().split('\n')
    i = 0
    
    # è·³è¿‡æ–‡ä»¶å¤´
    if lines and lines[0].startswith('#EXTM3U'):
        i = 1
    
    current_channel = None
    
    while i < len(lines):
        line = lines[i].strip()
        
        if line.startswith('#EXTINF'):
            # è§£æ EXTINF è¡Œ
            current_channel = {
                'extinf_line': line,
                'extra_lines': [],  # å­˜å‚¨ #EXTVLCOPT, #KODIPROP ç­‰é¢å¤–è¡Œ
                'attributes': {},
                'url': None,
                'title': '',
                'group_title': ''
            }
            
            # æå–æ ‡é¢˜ï¼ˆæœ€åä¸€ä¸ªé€—å·åçš„å†…å®¹ï¼‰
            if ',' in line:
                title_part = line.split(',')[-1]
                current_channel['title'] = title_part.strip()
            
            # è§£æå±æ€§ï¼ˆtvg-id, tvg-name, tvg-logo, group-title, http-referer ç­‰ï¼‰
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å±æ€§ï¼Œæ”¯æŒå¤šç§å±æ€§æ ¼å¼
            # åŒ¹é… key="value" æ ¼å¼
            attr_pattern = r'(\S+?)="([^"]*)"'
            matches = re.findall(attr_pattern, line)
            for attr_name, attr_value in matches:
                current_channel['attributes'][attr_name] = attr_value
                if attr_name == 'group-title':
                    current_channel['group_title'] = attr_value
            
        elif line.startswith('#') and not line.startswith('#EXTINF') and current_channel:
            # è¿™æ˜¯é¢å¤–çš„å±æ€§è¡Œï¼ˆå¦‚ #EXTVLCOPT, #KODIPROP ç­‰ï¼‰
            current_channel['extra_lines'].append(line)
            
        elif line and not line.startswith('#') and current_channel:
            # è¿™æ˜¯ URL è¡Œ
            current_channel['url'] = line.strip()
            
            # æ£€æŸ¥ group-title æ˜¯å¦åœ¨ç›®æ ‡åˆ—è¡¨ä¸­
            if current_channel['group_title'] in target_groups_set:
                # æ„å»ºç”¨äºé»‘åå•æ£€æŸ¥çš„é¢‘é“å¯¹è±¡ï¼ˆå…¼å®¹ is_blacklisted å‡½æ•°ï¼‰
                channel_for_check = {
                    'title': current_channel['title'],
                    'zteurl': current_channel['url']
                }
                
                # åº”ç”¨é»‘åå•è¿‡æ»¤
                if is_blacklisted(channel_for_check):
                    blacklisted_channels.append({
                        'title': current_channel['title'],
                        'code': '',
                        'reason': 'é»‘åå•è§„åˆ™åŒ¹é…',
                        'source': 'å¤–éƒ¨M3U'
                    })
                    current_channel = None
                    i += 1
                    continue
                
                # åˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸å‰¯æœ¬ï¼Œé¿å…å¼•ç”¨é—®é¢˜
                channel_copy = current_channel.copy()
                channel_copy['extra_lines'] = current_channel['extra_lines'].copy()
                channels.append(channel_copy)
            
            current_channel = None
        
        i += 1
    
    print(f"ä»å¤–éƒ¨ M3U ä¸­æå–äº† {len(channels)} ä¸ªé¢‘é“ (ç›®æ ‡åˆ†ç»„: {', '.join(target_groups)})")
    if blacklisted_channels:
        print(f"å·²è¿‡æ»¤ {len(blacklisted_channels)} ä¸ªé»‘åå•å¤–éƒ¨é¢‘é“")
    return channels, blacklisted_channels

def build_external_extinf_line(channel, use_proxy=True):
    """
    æ„å»ºå¤–éƒ¨é¢‘é“çš„ EXTINF è¡Œï¼Œåº”ç”¨ NGINX_PROXY_PREFIX åˆ° tvg-logo
    """
    # è·å–åŸå§‹ EXTINF è¡Œå’Œå±æ€§
    original_line = channel.get('extinf_line', '')
    attributes = channel.get('attributes', {})
    title = channel.get('title', '')
    
    # å¦‚æœæ²¡æœ‰ tvg-logo å±æ€§ï¼Œç›´æ¥è¿”å›åŸå§‹è¡Œ
    if 'tvg-logo' not in attributes:
        return original_line
    
    # å¤„ç† tvg-logoï¼Œåº”ç”¨ NGINX_PROXY_PREFIXï¼ˆå¦‚æœè®¾ç½®ä¸”å…è®¸ä½¿ç”¨ä»£ç†ï¼‰
    logo_url = attributes['tvg-logo']
    if use_proxy and NGINX_PROXY_PREFIX_NORM and logo_url:
        # æå–å›¾æ ‡çš„è·¯å¾„éƒ¨åˆ†
        if logo_url.startswith('http://'):
            logo_path = logo_url[7:]
        elif logo_url.startswith('https://'):
            logo_path = logo_url[8:]
        else:
            logo_path = logo_url
        
        # ç¡®ä¿è·¯å¾„ä¸ä»¥æ–œæ å¼€å¤´ï¼Œé¿å…é‡å¤æ–œæ 
        if logo_path.startswith('/'):
            logo_path = logo_path[1:]
        
        # ç»„åˆä»£ç†URL
        new_logo_url = NGINX_PROXY_PREFIX_NORM + logo_path
        
        # åœ¨åŸå§‹è¡Œä¸­æ›¿æ¢ tvg-logo çš„å€¼
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ tvg-logo="åŸå€¼" ä¸º tvg-logo="æ–°å€¼"
        logo_pattern = r'tvg-logo="([^"]*)"'
        new_line = re.sub(logo_pattern, f'tvg-logo="{new_logo_url}"', original_line)
        return new_line
    
    # å¦‚æœä¸éœ€è¦ä»£ç†ï¼Œè¿”å›åŸå§‹è¡Œ
    return original_line

def generate_m3u_content(grouped_channels, replace_url, catchup_template=CATCHUP_URL_TEMPLATE, external_channels=None, is_tv_m3u=False):
    """
    ç”Ÿæˆ M3U å†…å®¹
    :param grouped_channels: æœ¬åœ°é¢‘é“åˆ†ç»„å­—å…¸
    :param replace_url: æ˜¯å¦æ›¿æ¢ URLï¼ˆç»„æ’­è½¬å•æ’­ï¼‰
    :param catchup_template: å›çœ‹ URL æ¨¡æ¿
    :param external_channels: å¤–éƒ¨é¢‘é“åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™åˆå¹¶åˆ° M3U
    :param is_tv_m3u: æ˜¯å¦ä¸º tv.m3u æ–‡ä»¶ï¼ˆå½±å“ URL æ›¿æ¢ã€ä»£ç†ä½¿ç”¨å’Œ ztecode å‚æ•°ï¼‰
    :return: M3U æ–‡ä»¶å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
    """
    if M3U_EPG_URL:
        content = [f'#EXTM3U x-tvg-url="{M3U_EPG_URL}"']
    else:
        content = ["#EXTM3U"]
    
    catchup_enabled_count = 0
    
    # --- æ”¹è¿›çš„ä»£ç†å¤„ç†é€»è¾‘ï¼ˆä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œå…¼å®¹æ€§æ›´å¥½ï¼‰---
    # é»˜è®¤ä½¿ç”¨åŸå§‹å›çœ‹å‰ç¼€
    final_catchup_prefix = CATCHUP_SOURCE_PREFIX_NORM
    
    # å¦‚æœè®¾ç½®äº†ä»£ç†å‰ç¼€ï¼Œå¹¶ä¸”å›çœ‹å‰ç¼€ä¹Ÿå­˜åœ¨ï¼ˆtv.m3u éœ€è¦æ£€æŸ¥å¼€å…³ï¼‰
    use_proxy_for_this_file = NGINX_PROXY_PREFIX_NORM and (not is_tv_m3u or ENABLE_NGINX_PROXY_FOR_TV)
    if use_proxy_for_this_file and CATCHUP_SOURCE_PREFIX_NORM:
        # æå–å›çœ‹æºçš„è·¯å¾„éƒ¨åˆ†ï¼ˆå»é™¤åè®®å’ŒåŸŸåï¼‰
        if CATCHUP_SOURCE_PREFIX_NORM.startswith('http://'):
            catchup_path = CATCHUP_SOURCE_PREFIX_NORM[7:]
        elif CATCHUP_SOURCE_PREFIX_NORM.startswith('https://'):
            catchup_path = CATCHUP_SOURCE_PREFIX_NORM[8:]
        else:
            catchup_path = CATCHUP_SOURCE_PREFIX_NORM
        
        # ç¡®ä¿è·¯å¾„ä¸ä»¥æ–œæ å¼€å¤´ï¼Œé¿å…é‡å¤æ–œæ 
        if catchup_path.startswith('/'):
            catchup_path = catchup_path[1:]
        
        # ç»„åˆæˆæ–°çš„ä»£ç†å›çœ‹å‰ç¼€
        final_catchup_prefix = NGINX_PROXY_PREFIX_NORM + catchup_path
        print(f"å·²å°†å›çœ‹æºä»£ç†è‡³: {final_catchup_prefix}")
    
    # å¦‚æœæä¾›äº†å¤–éƒ¨é¢‘é“ï¼ŒæŒ‰åˆ†ç»„ç»„ç»‡å®ƒä»¬
    external_channels_by_group = {}
    external_groups_in_order = set()  # è®°å½•åœ¨ GROUP_OUTPUT_ORDER ä¸­çš„å¤–éƒ¨åˆ†ç»„
    external_groups_not_in_order = set()  # è®°å½•ä¸åœ¨ GROUP_OUTPUT_ORDER ä¸­çš„å¤–éƒ¨åˆ†ç»„
    
    if external_channels:
        for channel in external_channels:
            group_title = channel.get('group_title', '')
            if group_title not in external_channels_by_group:
                external_channels_by_group[group_title] = []
            external_channels_by_group[group_title].append(channel)
            
            # æ£€æŸ¥è¯¥åˆ†ç»„æ˜¯å¦åœ¨ GROUP_OUTPUT_ORDER ä¸­
            if group_title in GROUP_OUTPUT_ORDER:
                external_groups_in_order.add(group_title)
            else:
                external_groups_not_in_order.add(group_title)
    
    # (--- ä¿®æ”¹ï¼šä½¿ç”¨å…¨å±€è¾“å‡ºé¡ºåº ---)
    # æŒ‰ç…§ GROUP_OUTPUT_ORDER çš„é¡ºåºè¾“å‡ºæœ¬åœ°é¢‘é“ï¼Œå¹¶åœ¨å¯¹åº”ä½ç½®è¾“å‡ºå¤–éƒ¨é¢‘é“
    for group in GROUP_OUTPUT_ORDER:
        # å¦‚æœè¯¥åˆ†ç»„åœ¨ GROUP_OUTPUT_ORDER ä¸­ä¸”æœ‰å¤–éƒ¨é¢‘é“ï¼Œå…ˆè¾“å‡ºå¤–éƒ¨é¢‘é“
        if group in external_groups_in_order and group in external_channels_by_group:
            for channel in external_channels_by_group[group]:
                # æ„å»º EXTINF è¡Œï¼ˆåº”ç”¨ NGINX_PROXY_PREFIX åˆ° tvg-logoï¼Œtv.m3u éœ€è¦æ£€æŸ¥å¼€å…³ï¼‰
                extinf_line = build_external_extinf_line(channel, use_proxy_for_this_file)
                content.append(extinf_line)
                # æ·»åŠ é¢å¤–çš„å±æ€§è¡Œï¼ˆå¦‚ #EXTVLCOPT, #KODIPROP ç­‰ï¼‰
                for extra_line in channel.get('extra_lines', []):
                    content.append(extra_line)
                # å¤„ç†å¤–éƒ¨é¢‘é“ URLï¼Œåº”ç”¨ NGINX_PROXY_PREFIXï¼ˆå¦‚æœè®¾ç½®ï¼Œtv.m3u éœ€è¦æ£€æŸ¥å¼€å…³ï¼‰
                external_url = channel['url']
                if use_proxy_for_this_file and external_url:
                    # æå– URL çš„è·¯å¾„éƒ¨åˆ†
                    if external_url.startswith('http://'):
                        url_path = external_url[7:]
                    elif external_url.startswith('https://'):
                        url_path = external_url[8:]
                    else:
                        url_path = external_url
                    
                    # ç¡®ä¿è·¯å¾„ä¸ä»¥æ–œæ å¼€å¤´ï¼Œé¿å…é‡å¤æ–œæ 
                    if url_path.startswith('/'):
                        url_path = url_path[1:]
                    
                    # ç»„åˆä»£ç† URL
                    external_url = NGINX_PROXY_PREFIX_NORM + url_path
                # æ·»åŠ  URL
                content.append(external_url)
        
        # è¾“å‡ºæœ¬åœ°é¢‘é“
        for ch in grouped_channels.get(group, []):
            
            # è·³è¿‡æ²¡æœ‰æ’­æ”¾é“¾æ¥çš„é¢‘é“
            if not ch.get("zteurl"):
                continue
                    
# ä¿®å¤çš„URLæ›¿æ¢é€»è¾‘ - ä½¿ç”¨æ›´å¥å£®çš„æ–¹æ³•
            # tv.m3u ä½¿ç”¨ REPLACEMENT_IP_TVï¼ˆå¦‚æœé…ç½®ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹åœ°å€
            if is_tv_m3u and REPLACEMENT_IP_TV_NORM:
                # tv.m3u ä½¿ç”¨ä¸“ç”¨çš„ REPLACEMENT_IP_TVï¼Œå°†ç»„æ’­åœ°å€ä½œä¸º channel å‚æ•°
                # è·å–å‰ç¼€
                current_prefix = REPLACEMENT_IP_TV_NORM              
                # æ£€æµ‹å¹¶ä¿®å¤ï¼šå¦‚æœé…ç½®çš„æ˜¯ PHP å‚æ•° (å¦‚ channel=)ï¼Œnormalize_url ä¼šå°†å…¶å˜æˆ channel=/
                # è¿™é‡Œåˆ¤æ–­å¦‚æœå‰ç¼€ä»¥ =/ ç»“å°¾ï¼Œå°±ç§»é™¤æœ€åçš„ /
                if current_prefix.endswith('=/'):
                    current_prefix = current_prefix[:-1]
                # =========== ä¿®æ”¹ç»“æŸ ===========

                original_url = ch["zteurl"]
                parsed_original = urlparse(original_url)

                # æå–ç»„æ’­åœ°å€ï¼ˆIP:ç«¯å£ï¼‰ä½œä¸º channel å‚æ•°å€¼
                if parsed_original.scheme in ["rtp", "rtsp"]:
                    # å¯¹äº rtp://239.20.0.104:2006ï¼Œæå– 239.20.0.104:2006
                    address_part = parsed_original.netloc
                    # å¦‚æœæœ‰è·¯å¾„ï¼Œä¹ŸåŒ…å«è¿›å»ï¼ˆè™½ç„¶ç»„æ’­åœ°å€é€šå¸¸æ²¡æœ‰è·¯å¾„ï¼‰
                    if parsed_original.path:
                        address_part += parsed_original.path
                    # æ³¨æ„ï¼šè¿™é‡ŒæŠŠ REPLACEMENT_IP_TV_NORM æ¢æˆäº† current_prefix
                    url = current_prefix + address_part
                elif parsed_original.scheme in ["http", "https"]:
                    # å¯¹äº http/httpsï¼Œä¹Ÿæå– netloc ä½œä¸ºå‚æ•°
                    address_part = parsed_original.netloc
                    if parsed_original.path:
                        address_part += parsed_original.path
                    # æ³¨æ„ï¼šè¿™é‡ŒæŠŠ REPLACEMENT_IP_TV_NORM æ¢æˆäº† current_prefix
                    url = current_prefix + address_part
                elif not parsed_original.scheme:
                    # å¯¹äºæ²¡æœ‰ scheme çš„åœ°å€ï¼ˆå¦‚ "239.21.0.88:3692"ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                    # æ³¨æ„ï¼šè¿™é‡ŒæŠŠ REPLACEMENT_IP_TV_NORM æ¢æˆäº† current_prefix
                    url = current_prefix + original_url
                else:
                    # For unknown schemes (e.g., 'foo://...'), keep the original
                    url = original_url

            elif replace_url:
                # å…¶ä»–æ–‡ä»¶ä½¿ç”¨æ ‡å‡†çš„ REPLACEMENT_IP
                original_url = ch["zteurl"]
                parsed_original = urlparse(original_url)

                # We only want to replace known multicast/streaming protocols
                if parsed_original.scheme in ["rtp", "rtsp", "http", "https"]:
                    # The part to append is the netloc and the path
                    address_part = parsed_original.netloc + parsed_original.path
                    url = urljoin(REPLACEMENT_IP_NORM, address_part)
                elif not parsed_original.scheme:
                    # Handle cases like "239.21.0.88:3692" (no scheme)
                    url = urljoin(REPLACEMENT_IP_NORM, original_url)
                else:
                    # For unknown schemes (e.g., 'foo://...'), keep the original
                    url = original_url
            else:
                url = ch["zteurl"]
            
            # æ”¹è¿›çš„å›¾æ ‡URLå¤„ç†ï¼ˆä½¿ç”¨ç®€å•å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œå…¼å®¹æ€§æ›´å¥½ï¼‰
            logo_url = ch.get("icon", "")
            if logo_url:
                # å¦‚æœè®¾ç½®äº†ä»£ç†å‰ç¼€ï¼Œåˆ™é€šè¿‡ä»£ç†è®¿é—®ï¼ˆtv.m3u éœ€è¦æ£€æŸ¥å¼€å…³ï¼‰
                if use_proxy_for_this_file:
                    # æå–å›¾æ ‡çš„è·¯å¾„éƒ¨åˆ†
                    if logo_url.startswith('http://'):
                        logo_path = logo_url[7:]
                    elif logo_url.startswith('https://'):
                        logo_path = logo_url[8:]
                    else:
                        logo_path = logo_url
                    
                    # ç¡®ä¿è·¯å¾„ä¸ä»¥æ–œæ å¼€å¤´ï¼Œé¿å…é‡å¤æ–œæ 
                    if logo_path.startswith('/'):
                        logo_path = logo_path[1:]
                    
                    # ç»„åˆä»£ç†URL
                    logo_url = NGINX_PROXY_PREFIX_NORM + logo_path
                else:
                    # å¦‚æœæ²¡æœ‰ä»£ç†å‰ç¼€ï¼Œç¡®ä¿URLæœ‰åè®®å‰ç¼€
                    logo_url = ensure_url_scheme(logo_url)
            
            #  ä¿®æ”¹ï¼šä½¿ç”¨æ¸…ç†åçš„ tvg-id
            cleaned_tvg_id = clean_tvg_id(ch.get("original_title", ch["title"]))
            
            # æ„å»ºEXTINFè¡Œ
            extinf_parts = [
                f'#EXTINF:-1 tvg-id="{cleaned_tvg_id}"',
                f'tvg-name="{ch.get("original_title", ch["title"])}"',
                f'tvg-logo="{logo_url}"'
            ]
            
            # tv.m3u æ·»åŠ  ztecode å‚æ•°
            if is_tv_m3u:
                ztecode = ch.get("ztecode", "")
                if ztecode:
                    extinf_parts.append(f'ztecode="{ztecode}"')
            
            # åªæœ‰å½“é¢‘é“æ”¯æŒå›çœ‹æ—¶æ‰æ·»åŠ catchupå±æ€§
            if ch.get("supports_catchup", False):
                ztecode = ch.get("ztecode", "")
                if ztecode:
                    #  ä½¿ç”¨ä¼ å…¥çš„å›çœ‹URLæ¨¡æ¿
                    catchup_source = catchup_template.format(
                        prefix=final_catchup_prefix,
                        ztecode=ztecode
                    )
                    # ç¡®ä¿ç”Ÿæˆçš„catchup_sourceæœ‰åè®®å‰ç¼€ï¼ˆä¿é™©èµ·è§ï¼‰
                    catchup_source = ensure_url_scheme(catchup_source)
                    
                    extinf_parts.append(f'catchup="default"')
                    extinf_parts.append(f'catchup-source="{catchup_source}"')
                    catchup_enabled_count += 1
                elif ch.get("is_custom", False):
                    print(f"æç¤º: è‡ªå®šä¹‰é¢‘é“ '{ch['title']}' æ ‡è®°ä¸ºæ”¯æŒå›çœ‹ä½†ç¼ºå°‘ 'ztecode'ã€‚")
            
            extinf_parts.append(f'group-title="{group}",{ch["title"]}')
            
            content.append(' '.join(extinf_parts))
            content.append(url)
    
    # åœ¨æœ€åæ·»åŠ ä¸åœ¨ GROUP_OUTPUT_ORDER ä¸­çš„å¤–éƒ¨é¢‘é“ï¼ˆæŒ‰åˆ†ç»„ç»„ç»‡ï¼‰
    if external_groups_not_in_order:
        # æŒ‰ç…§åˆ†ç»„çš„å­—æ¯é¡ºåºæ’åºï¼Œç¡®ä¿è¾“å‡ºé¡ºåºç¨³å®š
        for group_title in sorted(external_groups_not_in_order):
            if group_title in external_channels_by_group:
                for channel in external_channels_by_group[group_title]:
                    # æ„å»º EXTINF è¡Œï¼ˆåº”ç”¨ NGINX_PROXY_PREFIX åˆ° tvg-logoï¼Œtv.m3u éœ€è¦æ£€æŸ¥å¼€å…³ï¼‰
                    extinf_line = build_external_extinf_line(channel, use_proxy_for_this_file)
                    content.append(extinf_line)
                    # æ·»åŠ é¢å¤–çš„å±æ€§è¡Œï¼ˆå¦‚ #EXTVLCOPT, #KODIPROP ç­‰ï¼‰
                    for extra_line in channel.get('extra_lines', []):
                        content.append(extra_line)
                    # å¤„ç†å¤–éƒ¨é¢‘é“ URLï¼Œåº”ç”¨ NGINX_PROXY_PREFIXï¼ˆå¦‚æœè®¾ç½®ï¼Œtv.m3u éœ€è¦æ£€æŸ¥å¼€å…³ï¼‰
                    external_url = channel['url']
                    if use_proxy_for_this_file and external_url:
                        # æå– URL çš„è·¯å¾„éƒ¨åˆ†
                        if external_url.startswith('http://'):
                            url_path = external_url[7:]
                        elif external_url.startswith('https://'):
                            url_path = external_url[8:]
                        else:
                            url_path = external_url
                        
                        # ç¡®ä¿è·¯å¾„ä¸ä»¥æ–œæ å¼€å¤´ï¼Œé¿å…é‡å¤æ–œæ 
                        if url_path.startswith('/'):
                            url_path = url_path[1:]
                        
                        # ç»„åˆä»£ç† URL
                        external_url = NGINX_PROXY_PREFIX_NORM + url_path
                    # æ·»åŠ  URL
                    content.append(external_url)
            
    print(f"å·²ä¸º {catchup_enabled_count} ä¸ªæ”¯æŒå›çœ‹çš„é¢‘é“æ·»åŠ catchupå±æ€§")
    return '\n'.join(content)

def main():
    # æ‰“å°å½“å‰ä½¿ç”¨çš„é…ç½®
    print_configuration()
    
    # åŠ è½½è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
    channel_order = load_channel_order(CHANNEL_ORDER_FILE)
    custom_channels_config = load_custom_channels(CUSTOM_CHANNELS_FILE)
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"è‡ªå®šä¹‰é¢‘é“é…ç½®: {list(custom_channels_config.keys())}")
    for group_name, channels in custom_channels_config.items():
        print(f"  åˆ†ç»„ '{group_name}' æœ‰ {len(channels)} ä¸ªé¢‘é“")

    data = download_json_data(JSON_URL)
    if data is None:
        print("ç¨‹åºé€€å‡º")
        sys.exit(1)

    channels = data["channels"]
    
    # å¤„ç†é¢‘é“ï¼ˆå»é‡ã€åç§°æ˜ å°„ç­‰ï¼‰
    kept_channels, blacklisted_main_channels, removed_channels = process_channels(channels)

    grouped_channels = {group: [] for group in GROUP_DEFINITIONS.keys()}


    skipped_url_count = 0 # ç”¨äºç»Ÿè®¡è·³è¿‡çš„é¢‘é“

    for channel in kept_channels:
        category = categorize_channel(channel["title"])
        
        # æ£€æŸ¥é¢‘é“æ˜¯å¦æ”¯æŒå›çœ‹åŠŸèƒ½
        supports_catchup = (channel.get("timeshiftAvailable", "false") == "true" or 
                           channel.get("lookbackAvailable", "false") == "true")
        
        # ä½¿ç”¨æœ€ç»ˆåç§°
        final_name = channel.get("final_name", channel["title"])
        
        # --- æ–°å¢ï¼šå¤„ç† zteurl å’Œ hwurl ---
        params = channel.get("params", {})
        zteurl = params.get("zteurl")
        hwurl = params.get("hwurl")
        
        final_url = ""
        if zteurl: 
            final_url = zteurl
        elif hwurl: 
            final_url = hwurl
        
        # å¦‚æœä¸¤ä¸ªURLéƒ½æ— æ•ˆï¼ˆä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼‰ï¼Œåˆ™è·³è¿‡æ­¤é¢‘é“
        if not final_url:
            skipped_url_count += 1
            continue
        
        grouped_channels[category].append({
            "title": final_name,
            "original_title": channel["title"],
            "code": channel["code"],
            "ztecode": params.get("ztecode", ""),
            "icon": channel["icon"],
            "zteurl": final_url,
            "number": extract_number(final_name),
            "supports_catchup": supports_catchup,
            "is_custom": False
        })

    # æ·»åŠ è‡ªå®šä¹‰é¢‘é“å¹¶è·å–é»‘åå•ä¿¡æ¯
    grouped_channels, blacklisted_custom_channels, added_custom_channels = add_custom_channels(grouped_channels, custom_channels_config)
    
    #  åˆå¹¶æ‰€æœ‰é»‘åå•é¢‘é“ï¼ˆå¤–éƒ¨é»‘åå•ä¼šåœ¨åé¢æ·»åŠ ï¼‰
    all_blacklisted_channels = blacklisted_main_channels + blacklisted_custom_channels
    
    # åº”ç”¨è‡ªå®šä¹‰æ’åº
    grouped_channels = apply_custom_sorting(grouped_channels, channel_order)
    
    # å¯¹äºæ²¡æœ‰è‡ªå®šä¹‰æ’åºçš„ç»„ï¼Œä½¿ç”¨é»˜è®¤æ’åº
    for category in grouped_channels:
        if category not in channel_order:
            grouped_channels[category].sort(key=lambda x: (x["number"], x["title"]))

    # ä¸‹è½½å¹¶è§£æå¤–éƒ¨ M3Uï¼ˆå¦‚æœå¯ç”¨åˆå¹¶ï¼‰
    external_channels = None
    blacklisted_external_channels = []
    if ENABLE_EXTERNAL_M3U_MERGE and EXTERNAL_M3U_URL and EXTERNAL_GROUP_TITLES:
        print(f"\nå¼€å§‹å¤„ç†å¤–éƒ¨ M3U åˆå¹¶...")
        external_m3u_content = download_external_m3u(EXTERNAL_M3U_URL)
        if external_m3u_content:
            external_channels, blacklisted_external_channels = parse_m3u_content(external_m3u_content, EXTERNAL_GROUP_TITLES)
            if external_channels:
                print(f"æˆåŠŸæå– {len(external_channels)} ä¸ªå¤–éƒ¨é¢‘é“ï¼Œå°†åˆå¹¶åˆ°æ‰€æœ‰ M3U æ–‡ä»¶")
                # æ£€æŸ¥å¤–éƒ¨åˆ†ç»„æ˜¯å¦åœ¨ GROUP_OUTPUT_ORDER ä¸­
                for external_group in EXTERNAL_GROUP_TITLES:
                    if external_group in GROUP_OUTPUT_ORDER:
                        print(f"å¤–éƒ¨åˆ†ç»„ '{external_group}' å·²åœ¨è¾“å‡ºé¡ºåºä¸­ï¼Œå°†æŒ‰é¡ºåºè¾“å‡º")
                    else:
                        print(f"å¤–éƒ¨åˆ†ç»„ '{external_group}' ä¸åœ¨è¾“å‡ºé¡ºåºä¸­ï¼Œå°†æ·»åŠ åˆ° M3U æ–‡ä»¶æœ«å°¾")
            else:
                print(f"è­¦å‘Š: ä»å¤–éƒ¨ M3U ä¸­æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„åˆ†ç»„é¢‘é“ï¼ˆç›®æ ‡åˆ†ç»„: {', '.join(EXTERNAL_GROUP_TITLES)}ï¼‰")
        else:
            print(f"è­¦å‘Š: æ— æ³•ä¸‹è½½å¤–éƒ¨ M3U æ–‡ä»¶ï¼Œè·³è¿‡å¤–éƒ¨é¢‘é“åˆå¹¶")
    elif ENABLE_EXTERNAL_M3U_MERGE:
        if not EXTERNAL_M3U_URL:
            print(f"æç¤º: ENABLE_EXTERNAL_M3U_MERGE å·²å¯ç”¨ï¼Œä½† EXTERNAL_M3U_URL æœªé…ç½®ï¼Œè·³è¿‡å¤–éƒ¨é¢‘é“åˆå¹¶")
        elif not EXTERNAL_GROUP_TITLES:
            print(f"æç¤º: ENABLE_EXTERNAL_M3U_MERGE å·²å¯ç”¨ï¼Œä½† EXTERNAL_GROUP_TITLES ä¸ºç©ºï¼Œè·³è¿‡å¤–éƒ¨é¢‘é“åˆå¹¶")
    
    # åˆå¹¶å¤–éƒ¨é»‘åå•é¢‘é“åˆ°æ€»é»‘åå•
    all_blacklisted_channels = blacklisted_main_channels + blacklisted_custom_channels + blacklisted_external_channels

    #  ç”ŸæˆM3Uæ–‡ä»¶ - ç°åœ¨ç”Ÿæˆä¸‰ä¸ªæ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨äº†å¤–éƒ¨åˆå¹¶ï¼Œæ‰€æœ‰æ–‡ä»¶éƒ½ä¼šåŒ…å«å¤–éƒ¨é¢‘é“ï¼‰
    for filename, replace_url, catchup_template, is_tv_m3u in [
        (TV_M3U_FILENAME, False, CATCHUP_URL_TEMPLATE, True),      # ç»„æ’­åœ°å€ï¼Œæ ‡å‡†å›çœ‹æ¨¡æ¿ï¼Œtv.m3u ç‰¹æ®Šå¤„ç†
        (TV2_M3U_FILENAME, True, CATCHUP_URL_TEMPLATE, False),      # å•æ’­åœ°å€ï¼Œæ ‡å‡†å›çœ‹æ¨¡æ¿
        (KU9_M3U_FILENAME, True, CATCHUP_URL_KU9, False)           #  å•æ’­åœ°å€ï¼ŒKU9å›çœ‹æ¨¡æ¿
    ]:
        content = generate_m3u_content(grouped_channels, replace_url, catchup_template, external_channels, is_tv_m3u)
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        external_count = len(external_channels) if external_channels else 0
        if external_count > 0:
            print(f"å·²ç”ŸæˆM3Uæ–‡ä»¶: {filename} (åŒ…å« {external_count} ä¸ªå¤–éƒ¨é¢‘é“)")
        else:
            print(f"å·²ç”ŸæˆM3Uæ–‡ä»¶: {filename}")

    total_channels = sum(len(v) for v in grouped_channels.values())
    external_count = len(external_channels) if external_channels else 0
    total_channels_with_external = total_channels + external_count
    
    #  æ›´æ–°ç»Ÿè®¡è¾“å‡º
    print(f"\nå·²è·³è¿‡ {skipped_url_count} ä¸ªç¼ºå°‘æ’­æ”¾é“¾æ¥çš„é¢‘é“ã€‚")
    blacklist_info_parts = [f"ä¸»JSON: {len(blacklisted_main_channels)}", f"è‡ªå®šä¹‰: {len(blacklisted_custom_channels)}"]
    if blacklisted_external_channels:
        blacklist_info_parts.append(f"å¤–éƒ¨: {len(blacklisted_external_channels)}")
    print(f"æ€»å…±è¿‡æ»¤ {len(all_blacklisted_channels)} ä¸ªé»‘åå•é¢‘é“ï¼ˆ{', '.join(blacklist_info_parts)}ï¼‰")
    
    if external_count > 0:
        print(f"æˆåŠŸç”Ÿæˆ {total_channels} ä¸ªæœ¬åœ°é¢‘é“ + {external_count} ä¸ªå¤–éƒ¨é¢‘é“ = æ€»è®¡ {total_channels_with_external} ä¸ªé¢‘é“")
    else:
        print(f"æˆåŠŸç”Ÿæˆ {total_channels} ä¸ªé¢‘é“")
    print(f"å•æ’­åœ°å€åˆ—è¡¨: {os.path.abspath(TV2_M3U_FILENAME)}")
    print(f"KU9å›çœ‹å‚æ•°åˆ—è¡¨: {os.path.abspath(KU9_M3U_FILENAME)}")  #  æ–°å¢è¾“å‡ºä¿¡æ¯
    
    # ç»Ÿä¸€ç”Ÿæˆå®Œæ•´çš„æ—¥å¿—æ–‡ä»¶ï¼ŒåŒ…å«ä¸»JSONå’Œè‡ªå®šä¹‰é¢‘é“çš„æ‰€æœ‰å¤„ç†ç»“æœ
    with open(CHANNEL_PROCESSING_LOG, "w", encoding="utf-8") as f:
        f.write("é¢‘é“å¤„ç†æ—¥å¿—\n")
        f.write(f"{LOG_SEPARATOR}\n\n")
        
        # ========== ä¸»JSONé¢‘é“å¤„ç†ç»“æœ ==========
        f.write("ã€ä¸»JSONé¢‘é“å¤„ç†ç»“æœã€‘\n")
        f.write(f"{LOG_SEPARATOR}\n\n")
        
        f.write(f"1. é»‘åå•è¿‡æ»¤ ({len(blacklisted_main_channels)} ä¸ª):\n")
        for channel in blacklisted_main_channels:
            f.write(f"  - æ ‡é¢˜: {channel['title']}, ä»£ç : {channel['code']}, åŸå› : {channel['reason']}\n")
        f.write("\n")
        
        f.write(f"2. å»é‡è¿‡æ»¤ ({len(removed_channels)} ä¸ª):\n")
        for channel in removed_channels:
            f.write(f"  - {channel['name']} (åŸå› : {channel['reason']})\n")
        f.write("\n")
        
        f.write(f"3. æœ€ç»ˆä¿ç•™ ({len(kept_channels)} ä¸ª):\n")
        for channel in kept_channels:
            original_name = channel["title"]
            final_name = channel.get("final_name", original_name)
            if original_name != final_name:
                f.write(f"  - {original_name} -> {final_name}\n")
            else:
                f.write(f"  - {original_name}\n")
        f.write("\n\n")
        
        # ========== è‡ªå®šä¹‰é¢‘é“å¤„ç†ç»“æœ ==========
        f.write("ã€è‡ªå®šä¹‰é¢‘é“å¤„ç†ç»“æœã€‘\n")
        f.write(f"{LOG_SEPARATOR}\n\n")
        
        f.write(f"1. é»‘åå•è¿‡æ»¤ ({len(blacklisted_custom_channels)} ä¸ª):\n")
        if blacklisted_custom_channels:
            for channel in blacklisted_custom_channels:
                f.write(f"  - æ ‡é¢˜: {channel['title']}, ä»£ç : {channel['code']}, åŸå› : {channel['reason']}\n")
        else:
            f.write("  (æ— )\n")
        f.write("\n")
        
        f.write(f"2. æˆåŠŸæ·»åŠ  ({len(added_custom_channels)} ä¸ª):\n")
        if added_custom_channels:
            for channel in added_custom_channels:
                original_name = channel['original_title']
                final_name = channel['title']
                group_name = channel['group']
                if original_name != final_name:
                    f.write(f"  - [{group_name}] {original_name} -> {final_name}\n")
                else:
                    f.write(f"  - [{group_name}] {final_name}\n")
        else:
            f.write("  (æ— )\n")
        f.write("\n\n")
        
        # ========== å¤–éƒ¨é¢‘é“å¤„ç†ç»“æœ ==========
        if ENABLE_EXTERNAL_M3U_MERGE:
            f.write("ã€å¤–éƒ¨ M3U é¢‘é“å¤„ç†ç»“æœã€‘\n")
            f.write(f"{LOG_SEPARATOR}\n\n")
            
            f.write(f"1. é»‘åå•è¿‡æ»¤ ({len(blacklisted_external_channels)} ä¸ª):\n")
            if blacklisted_external_channels:
                for channel in blacklisted_external_channels:
                    f.write(f"  - æ ‡é¢˜: {channel['title']}, åŸå› : {channel['reason']}\n")
            else:
                f.write("  (æ— )\n")
            f.write("\n")
            
            f.write(f"2. æˆåŠŸåˆå¹¶ ({external_count} ä¸ª):\n")
            if external_channels:
                for channel in external_channels:
                    f.write(f"  - [{channel.get('group_title', 'æœªçŸ¥åˆ†ç»„')}] {channel['title']}\n")
            else:
                f.write("  (æ— )\n")
            f.write("\n\n")
        
        # ========== æ±‡æ€»ä¿¡æ¯ ==========
        f.write("ã€å¤„ç†æ±‡æ€»ã€‘\n")
        f.write(f"{LOG_SEPARATOR}\n\n")
        f.write(f"é»‘åå•è¿‡æ»¤æ±‡æ€»:\n")
        f.write(f"  - ä¸»JSONé¢‘é“: {len(blacklisted_main_channels)} ä¸ª\n")
        f.write(f"  - è‡ªå®šä¹‰é¢‘é“: {len(blacklisted_custom_channels)} ä¸ª\n")
        if blacklisted_external_channels:
            f.write(f"  - å¤–éƒ¨é¢‘é“: {len(blacklisted_external_channels)} ä¸ª\n")
        f.write(f"  - æ€»è®¡: {len(all_blacklisted_channels)} ä¸ª\n")
        f.write("\n")
        f.write(f"æœ€ç»ˆé¢‘é“ç»Ÿè®¡:\n")
        f.write(f"  - ä¸»JSONä¿ç•™: {len(kept_channels)} ä¸ª\n")
        f.write(f"  - è‡ªå®šä¹‰é¢‘é“: {len(added_custom_channels)} ä¸ª\n")
        if external_count > 0:
            f.write(f"  - å¤–éƒ¨é¢‘é“: {external_count} ä¸ª\n")
            f.write(f"  - æ€»è®¡: {total_channels_with_external} ä¸ª\n")
        else:
            f.write(f"  - æ€»è®¡: {len(kept_channels) + len(added_custom_channels)} ä¸ª\n")
    
    print(f"å·²ç”Ÿæˆå¤„ç†æ—¥å¿—: {os.path.abspath(CHANNEL_PROCESSING_LOG)}")
    
    # --- EPG ä¸‹è½½æ§åˆ¶å¼€å…³ ---
    # é€šè¿‡é…ç½®åŒºåŸŸçš„ ENABLE_EPG_DOWNLOAD å¼€å…³æ§åˆ¶æ˜¯å¦ä¸‹è½½EPG
    if ENABLE_EPG_DOWNLOAD:
        run_epg_download(channels, custom_channels_config, grouped_channels)
    else:
        print("\nEPGä¸‹è½½å·²ç¦ç”¨ï¼Œè·³è¿‡EPGä¸‹è½½å’Œç”Ÿæˆã€‚")

if __name__ == "__main__":
    main()